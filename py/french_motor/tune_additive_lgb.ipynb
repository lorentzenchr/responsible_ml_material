{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized search CV for the additive LightGBM model\n",
    "\n",
    "- Rerunning takes long\n",
    "- Results might depend on seed\n",
    "- Basically a copy of Chapter 3 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning takes ~30 minutes; results might depend on seed\n",
    "import json\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GroupKFold, ParameterSampler  # , ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "grid_file = \"grid_add_lgb.txt\"\n",
    "\n",
    "train, test, X_train, X_test, y_train, y_test, w_train, w_test, xvars, prep_lgb = load(\n",
    "    \"data.joblib\"\n",
    ")\n",
    "\n",
    "# Data interface of LightGBM\n",
    "dtrain = lgb.Dataset(\n",
    "    prep_lgb.fit_transform(X_train),\n",
    "    label=y_train,\n",
    "    weight=w_train,\n",
    "    params={\"feature_pre_filter\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Model for expected claims frequency -> Poisson deviance\n",
    "\n",
    "# STEP 2: Select learning rate so that optimal number of rounds by early stopping is\n",
    "# somewhere between 100 and 1000\n",
    "params = {\n",
    "    \"objective\": \"poisson\",\n",
    "    \"learning_rate\": 0.5,  # much higher than for a more flexibel model\n",
    "    \"num_leaves\": 2,  # to get additive model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold grouped cross-validation to see how many trees are required by early stopping\n",
    "folds = list(GroupKFold(n_splits=5).split(X=X_train, groups=train.group_id))\n",
    "\n",
    "cvm = lgb.cv(\n",
    "    params=params,\n",
    "    train_set=dtrain,\n",
    "    num_boost_round=5000,\n",
    "    folds=folds,\n",
    "    # nfold = 5,  # for the ungrouped case, plus stratified=False,\n",
    "    eval_train_metric=True,\n",
    "    callbacks=[lgb.log_evaluation(period=0), lgb.early_stopping(20, verbose=0)],\n",
    ")\n",
    "\n",
    "# A LR of 0.5 provides about 400 trees, which is convenient\n",
    "cvm = pd.DataFrame(cvm)\n",
    "print(\"Best boosting round with default params:\\n\", len(cvm))\n",
    "cvm.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Iterate randomized SearchCV for regularization parameters\n",
    "if True:\n",
    "    # Or ParameterGrid(...) if grid is small enough to check all combinations\n",
    "    grid = ParameterSampler(\n",
    "        {\n",
    "            \"objective\": [\"poisson\"],\n",
    "            \"learning_rate\": [0.5],\n",
    "            \"num_leaves\": [2],  # to get additive model\n",
    "            \"reg_lambda\": [0, 2.5, 5, 7.5],\n",
    "            \"reg_alpha\": [0, 4],\n",
    "            \"colsample_bynode\": [0.8, 1],\n",
    "            \"subsample\": [0.8, 1],\n",
    "            \"min_child_samples\": [20, 50, 100],\n",
    "            \"poisson_max_delta_step\": [0.1, 0.7],\n",
    "            \"verbose\": [-1],\n",
    "        },\n",
    "        n_iter=50,\n",
    "        random_state=94,\n",
    "    )\n",
    "\n",
    "    # Iterate over grid and save relevant information on disk\n",
    "    search = []\n",
    "    for g in tqdm(grid):\n",
    "        cvm = lgb.cv(\n",
    "            params=g,\n",
    "            train_set=dtrain,\n",
    "            num_boost_round=5000,\n",
    "            folds=folds,\n",
    "            seed=82,\n",
    "            eval_train_metric=True,\n",
    "            callbacks=[lgb.log_evaluation(period=0), lgb.early_stopping(20, verbose=0)],\n",
    "        )\n",
    "        # Keep number of rounds, cv score, train score, and parameters\n",
    "        cvm = pd.DataFrame(cvm)\n",
    "        search.append((len(cvm), *cvm.iloc[-1, [2, 0]], g))\n",
    "\n",
    "        with open(grid_file, \"w\") as f:\n",
    "            json.dump(search, f)\n",
    "\n",
    "# Load grid and check (A) sort order and (B) if grid ranges were set reasonable\n",
    "with open(grid_file) as f:\n",
    "    search = json.load(f)\n",
    "\n",
    "search_df = pd.DataFrame.from_records(\n",
    "    search, columns=[\"num_boost_round\", \"cv_score\", \"train_score\", \"params\"]\n",
    ").sort_values(\"cv_score\")\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(search_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568,\n",
       " {'verbose': -1,\n",
       "  'subsample': 1,\n",
       "  'reg_lambda': 7.5,\n",
       "  'reg_alpha': 0,\n",
       "  'poisson_max_delta_step': 0.7,\n",
       "  'objective': 'poisson',\n",
       "  'num_leaves': 2,\n",
       "  'min_child_samples': 50,\n",
       "  'learning_rate': 0.5,\n",
       "  'colsample_bynode': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "best = search_df.iloc[0]\n",
    "best.num_boost_round, best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on best parameters\n",
    "lgbr = LGBMRegressor(**best.params, n_estimators=best.num_boost_round, random_state=59)\n",
    "model_lgb = Pipeline([(\"preprocessor\", prep_lgb), (\"model\", lgbr)])\n",
    "_ = model_lgb.fit(X=X_train, y=y_train, model__sample_weight=w_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
