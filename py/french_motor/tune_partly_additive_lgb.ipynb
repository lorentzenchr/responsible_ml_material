{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized search CV for the partly additive LightGBM model\n",
    "\n",
    "- Rerunning takes long\n",
    "- Results might depend on seed\n",
    "- Basically a copy of Chapter 3 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerunning takes ~30 minutes; results might depend on seed\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GroupKFold, ParameterSampler  # , ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "grid_file = \"grid_part_add_lgb.txt\"\n",
    "\n",
    "train, test, X_train, X_test, y_train, y_test, w_train, w_test, xvars, prep_lgb = load(\n",
    "    \"data.joblib\"\n",
    ")\n",
    "\n",
    "# Data interface of LightGBM\n",
    "dtrain = lgb.Dataset(\n",
    "    prep_lgb.fit_transform(X_train),\n",
    "    label=y_train,\n",
    "    weight=w_train,\n",
    "    params=dict(feature_pre_filter=False),\n",
    ")\n",
    "\n",
    "# Interaction constraints (mind the order of feature names after preprocessor)\n",
    "interaction_constraints = [[0, 1, 2, 5], [3], [4], [6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Model for expected claims frequency -> Poisson deviance\n",
    "\n",
    "# STEP 2: Select learning rate so that optimal number of rounds by early stopping is\n",
    "# somewhere between 100 and 1000\n",
    "params = dict(\n",
    "    objective=\"poisson\",\n",
    "    learning_rate=0.05,  # play with me\n",
    "    interaction_constraints=interaction_constraints,  # partly additive model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold grouped cross-validation to see how many trees are required by early stopping\n",
    "folds = list(GroupKFold(n_splits=5).split(X=X_train, groups=train.group_id))\n",
    "\n",
    "cvm = lgb.cv(\n",
    "    params=params,\n",
    "    train_set=dtrain,\n",
    "    num_boost_round=5000,\n",
    "    folds=folds,\n",
    "    # nfold = 5,  # for the ungrouped case, plus stratified=False,\n",
    "    eval_train_metric=True,\n",
    "    callbacks=[lgb.log_evaluation(period=0), lgb.early_stopping(20, verbose=0)],\n",
    ")\n",
    "\n",
    "# A LR of 0.05 provides about 400 trees, which is convenient\n",
    "cvm = pd.DataFrame(cvm)\n",
    "print(\"Best boosting round with default params:\\n\", len(cvm))\n",
    "cvm.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Iterate randomized SearchCV for regularization parameters\n",
    "if True:\n",
    "    # Or ParameterGrid(...) if grid is small enough to check all combinations\n",
    "    grid = ParameterSampler(\n",
    "        dict(\n",
    "            objective=[\"poisson\"],\n",
    "            learning_rate=[0.05],\n",
    "            num_leaves=[15, 31, 63],\n",
    "            interaction_constraints=[interaction_constraints],  # partly additive model\n",
    "            reg_lambda=[0, 2.5, 5, 7.5],\n",
    "            reg_alpha=[0, 4],\n",
    "            colsample_bynode=[0.8, 1],\n",
    "            subsample=[0.8, 1],\n",
    "            min_child_samples=[20, 50, 100],\n",
    "            poisson_max_delta_step=[0.1, 0.7],\n",
    "            verbose=[-1],\n",
    "        ),\n",
    "        n_iter=50,\n",
    "        random_state=94,\n",
    "    )\n",
    "\n",
    "    # Iterate over grid and save relevant information on disk\n",
    "    search = []\n",
    "    for g in tqdm(grid):\n",
    "        cvm = lgb.cv(\n",
    "            params=g,\n",
    "            train_set=dtrain,\n",
    "            num_boost_round=5000,\n",
    "            folds=folds,\n",
    "            seed=82,\n",
    "            eval_train_metric=True,\n",
    "            callbacks=[lgb.log_evaluation(period=0), lgb.early_stopping(20, verbose=0)],\n",
    "        )\n",
    "        # Keep number of rounds, cv score, train score, and parameters\n",
    "        cvm = pd.DataFrame(cvm)\n",
    "        search.append((len(cvm), *cvm.iloc[-1, [2, 0]], g))\n",
    "\n",
    "        with open(grid_file, \"w\") as f:\n",
    "            json.dump(search, f)\n",
    "\n",
    "# Load grid and check (A) sort order and (B) if grid ranges were set reasonable\n",
    "with open(grid_file) as f:\n",
    "    search = json.load(f)\n",
    "\n",
    "search_df = pd.DataFrame.from_records(\n",
    "    search, columns=[\"num_boost_round\", \"cv_score\", \"train_score\", \"params\"]\n",
    ").sort_values(\"cv_score\")\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(search_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358,\n",
       " {'verbose': -1,\n",
       "  'subsample': 1,\n",
       "  'reg_lambda': 7.5,\n",
       "  'reg_alpha': 4,\n",
       "  'poisson_max_delta_step': 0.1,\n",
       "  'objective': 'poisson',\n",
       "  'num_leaves': 15,\n",
       "  'min_child_samples': 100,\n",
       "  'learning_rate': 0.05,\n",
       "  'interaction_constraints': [[0, 1, 2, 5], [3], [4], [6]],\n",
       "  'colsample_bynode': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "best = search_df.iloc[0]\n",
    "best.num_boost_round, best.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\responsible_ml_lecture\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\responsible_ml_lecture\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "# Fit on best parameters\n",
    "lgbr = LGBMRegressor(**best.params, n_estimators=best.num_boost_round, random_state=59)\n",
    "model_lgb = Pipeline([(\"preprocessor\", prep_lgb), (\"model\", lgbr)])\n",
    "_ = model_lgb.fit(X=X_train, y=y_train, model__sample_weight=w_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
