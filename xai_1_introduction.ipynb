{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e41ea31c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the part \"XAI\" (e**X**plainable **A**rtificial **I**ntelligence) of the lecture \"Responsible Machine Learning with Insurance Applications\".\n",
    "\n",
    "The basic question of XAI is:\n",
    "\n",
    "> How to explain and interpret a given model, even if it seems a black box?\n",
    "\n",
    "Answering this question is a key aspect of responsible machine learning (ML). It not only provides valuable information and knowledge for stakeholders, but also helps to identify problems in the modeling process.\n",
    "\n",
    "# Scope and Taxonomy\n",
    "\n",
    "Thanks to the rise of ML and the desire to explain its models, the field of explainable ML is developing rapidly. We will study the most important methods for *structured data*. Unstructured data (images, audio sequences, text, ...) often has its own specialized explanation tools. Covering them is beyond the scope of this lecture.\n",
    "\n",
    "The following taxonomy of explainability methods is often helpful, see @molnar2019 for more background:\n",
    "\n",
    "-   *Global versus local explainability:* Global explainability methods describe the model as a whole, while local methods focus on explaining model behavior around a single observation. We will mainly cover global methods, with one notable exception (SHAP).\n",
    "-   *Model-specific versus model-agnostic:* Each modeling technique typically has its own tailored methods for interpretation. For example, a linear regression can be explained by examining its coefficients. Tree-based models are another example: The variable importance of a feature in such a model can be assessed, for instance, by studying the average loss reduction from splits on that feature. In contrast to such model-specific methods, a model-agnostic method is suitable for all types of models. Our focus is on model-agnostic techniques, although we sometimes show model-specific methods for comparison.\n",
    "-   *Intrinsic versus post-hoc*: The simple structure of a linear model, an additive model, or a short decision tree makes such modeling techniques intrinsically interpretable. In contrast, models with complex structure (e.g., a neural net) are interpretable only by post-hoc analysis of the fitted model. Model-agnostic interpretability methods are always used for post-hoc explainability. However, they can also be applied to an intrinsically interpretable model.\n",
    "\n",
    "Note: We will use the terms \"explainable\", \"interpretable\", and \"intelligible\" interchangeably, although subtle differences are sometimes pointed out in the literature.\n",
    "\n",
    "# Outline\n",
    "\n",
    "This chapter introduces important notations, the necessary background on non-life insurance pricing, and the main example that will guide us on our way through the illuminating world of model explainability.\n",
    "\n",
    "In the chapter \"Explaining Models\" we will learn the most important methods of post-hoc interpretability. We will also learn how to use the insights gained from interpreting complex models to improve linear models.\n",
    "\n",
    "The third chapter, \"Improving Explainability\", takes a different path. Here we explore ways to improve the intrinsic explainability of complex models by simplifying their internal structure, in particular by forcing parts of the model to be additive and/or monotonic.\n",
    "\n",
    "# Notation\n",
    "\n",
    "Throughout the XAI notebooks, we consider a typical modeling situation: a distributional property $ T $ of a real-valued response variable $Y$ should be approximated by a model $ m: \\boldsymbol x \\in \\mathbb R^p \\mapsto \\mathbb R $ of a $ p $-dimensional feature vector $ \\boldsymbol X = (X^{(1)}, \\dots, X^{(p)}) $ with value $ \\boldsymbol x = (x^{(1)}, \\dots, x^{(p)}) \\in \\mathbb R^p $, i.e., $$\n",
    "  T(Y\\mid \\boldsymbol X = \\boldsymbol x) \\approx m(\\boldsymbol x).$$ \n",
    "For brevity, we write $ T(Y\\mid \\boldsymbol X = \\boldsymbol x) = T(Y\\mid \\boldsymbol x) $.\n",
    "\n",
    "Examples of $T$ are the expectation $T =\\mathbb E$, or a quantile $q_\\alpha$. The model $m$ is then estimated by $\\hat m$ from the training data by minimizing some objective criterion typically of the form $$\n",
    "  \\frac{1}{\\sum_{i = 1}^n w_i} \\sum_{i = 1}^n w_i L(\\hat y_i, y_i) + \\lambda \\Omega(m),$$ where\n",
    "\n",
    "-   $ L $ is a loss function (sometimes called \"scoring function\") ideally strictly consistent for $T$, e.g., the squared error $L(z, y) = (y - z)^2$,\n",
    "-   $1 \\le i \\le n$ are the observations in the dataset considered,\n",
    "-   $\\lambda \\Omega(m)$ is an optional penalty to reduce overfitting,\n",
    "-   $\\boldsymbol w = (w_1, \\dots, w_n)^T$ is the vector of (optional) non-negative case weights of the $n$ observations ($w_i = 1$ if there are no such case weights),\n",
    "-   $\\boldsymbol y = (y_1, \\dots, y_n)^T$ are the $n$ observed values of $Y$,\n",
    "-   $\\boldsymbol{\\hat y} = (\\hat y_1, \\dots, \\hat y_n)^T$ is the vector of predicted or fitted values, i.e., $\\hat y_i = \\hat m(\\boldsymbol x_i)$,\n",
    "-   $\\boldsymbol x_1, \\dots, \\boldsymbol x_n$ are the feature vectors corresponding to the $n$ observations. Consequently, $x_i^{(j)}$ denotes the value of the $j$-th feature of the $i$-th observation.\n",
    "\n",
    "Even if many of the concepts covered in this lecture also work for classification settings with more than two classes, we focus on regression and binary classification.\n",
    "\n",
    "Some additional notation is introduced in the following examples of important model classes.\n",
    "\n",
    "## Example: Linear regression\n",
    "\n",
    "The model equation of a linear regression model is assumed to satisfy $$\n",
    "  \\mathbb E(Y \\mid \\boldsymbol x) = m(\\boldsymbol x) = \\beta_o + \\beta_1 x^{(1)} + \\dots + \\beta_p x^{(p)},$$ where $(\\beta_o, \\beta_1, \\dots, \\beta_p) \\in \\mathbb R^{p+1}$ is the parameter vector to be estimated by minimizing the sum of squared errors $$\n",
    "  \\sum_{i = 1}^n (y_i - \\hat y_i)^2$$ by linear least-squares.\n",
    "\n",
    "## Example: Generalized linear model (GLM)\n",
    "\n",
    "The model equation of a generalized linear model [@nelder1972] with link function $g$ and inverse link $g^{-1}$ is assumed to satisfy: $$\n",
    "  \\mathbb E(Y \\mid \\boldsymbol x) = m(\\boldsymbol x) = g^{-1}(\\eta(\\boldsymbol x)) = g^{-1}(\\beta_o + \\beta_1 x^{(1)} + \\dots + \\beta_p x^{(p)}),$$ where the linear part $\\eta$ of the model is called the *linear predictor*. The parameters $\\beta_j$ are estimated by minimizing the (possibly weighted) average deviance $\\bar S$, a measure depending on the distribution assumed for $Y\\mid \\boldsymbol x$. Typical distributions are the Poisson distribution, the Gamma distribution, the Bernoulli distribution, and the normal distribution.\n",
    "\n",
    "In this lecture, we will often model insurance claim counts by a Poisson GLM with log link $g$. Its (possibly weighted) average deviance on the training data $D_{\\text{train}} = \\{(y_i, w_i, \\boldsymbol x_i), i = 1, \\dots, n\\}$ is given by the formula $$\n",
    "   \\bar S(\\hat m, D_{\\text{train}}) = \\sum_{i = 1}^n w_i S(\\hat y_i, y_i) / \\sum_{i = 1}^n w_i,$$ where $S$ is the (unit) Poisson deviance of a single observation, $$\n",
    "  S(\\hat y_i, y_i) = 2(y_i \\log(y_i / \\hat y_i) - (y_i - \\hat y_i)).$$\n",
    "\n",
    "## Example: Generalized additive model (GAM)\n",
    "\n",
    "An important extension of the GLM is the generalized additive model [@hastie1986; @hastie1990; @wood2017]. Its model equation is assumed to satisfy $$\n",
    "  \\mathbb E(Y \\mid \\boldsymbol x) = m(\\boldsymbol x) = g^{-1}(\\beta_o + f_1(x^{(1)}) + \\dots + f_p(x^{(p)}))$$ for sufficiently nice functions $f_j$ describing the Ceteris Paribus effect of feature $X^{(j)}$ on $g(\\mathbb E(Y \\mid \\boldsymbol x))$, and $\\beta_o$ is the intercept. As functions $f_j$, often penalized regression smooths are used that are then being estimated to minimize the distribution specific average deviance. The functions $f_j$ are usually zero-centered to uniquely define them. Some of the components $f_j$ can also be fully parametric, just like in a GLM.\n",
    "\n",
    "GAMs and GLMs can also contain interaction terms, i.e., components that depend on more than one covariate.\n",
    "\n",
    "## Example: Gradient boosted decision trees (GBDT)\n",
    "\n",
    "Like a neural net, gradient boosted trees [@friedman2001] allow to represent extremely flexible model equations. They can automatically pick up complex interactions between covariates, a process that would need to be done in a manual way for linear models. Such models are fitted by applying gradient boosting on the objective function chosen in line with $T$.\n",
    "\n",
    "In the statistical software R, different implementation of gradient boosting exist. We will mainly work with \"XGBoost\" [@chen2016] and \"LightGBM\" [@ke2017]. They allow to model different functionals $T$, and offer a rich set of objective functions and many regularization parameters. We will, as a competitor of the Poisson GLM and the Poisson GAM, also consider Poisson boosted trees.\n",
    "\n",
    "For details on gradient boosting, check our corresponding lecture slides, and also [this section](https://mayer79.github.io/statistical_computing_material/5_Trees.html#4_Gradient_Boosted_Trees) of \"Statistical Computing\".\n",
    "\n",
    "# Non-Life Insurance Pricing\n",
    "\n",
    "The models described above are highly relevant in insurance pricing. A brief description of this field is given here.\n",
    "\n",
    "One of the main tasks of a non-life pricing actuary is to predict the *pure premium* associated with an insurance policy. It is defined as the financial loss per year or per some other relevant exposure measure. The insurance company uses this information to optimize its tariffs and to estimate expected future profit. Such predictions are usually made by statistical models based on historical data on policies and claims.\n",
    "\n",
    "Slightly adapting the notation of @wuethrich2021, we use the following terms to characterize an insurance policy with exposure $w$. Throughout these notebooks, $w$ is measured in yearly units.\n",
    "\n",
    "-   $w > 0$: The exposure. All other quantities will refer to this.\n",
    "-   $N$: Number of claims.\n",
    "-   $C$: Total claim amount.\n",
    "-   $C / w$: Pure premium.\n",
    "-   $Y = N / w$: The claims frequency, i.e., the number of claims per exposure.\n",
    "-   $Z = C / N$: The claims severity, i.e., the average cost per claim (undefined if $N = 0$).\n",
    "-   $\\boldsymbol X$: One or more risk characteristics describing the policy.\n",
    "\n",
    "Observed values for two fictive motor third-part liability (MTPL) policies could look like this:\n",
    "\n",
    "| id  | $w$ | $N$ | $C$  | $C/w$ | $Y$ | $Z$  | Driver's age | Horse power |\n",
    "|:---:|:---:|:---:|:----:|:-----:|:---:|:----:|:------------:|:-----------:|\n",
    "|  1  |  1  |  0  |  0   |   0   |  0  |  \\-  |      28      |     80      |\n",
    "|  2  | 0.5 |  2  | 5000 | 10000 |  4  | 2500 |      20      |     250     |\n",
    "|  2  | 0.5 |  1  | 1000 | 2000  |  2  | 1000 |      21      |     250     |\n",
    "\n",
    "Due to the additivity of $w$, $N$, and $C$, these quantities can also be defined for multiple policies together, e.g., for the entire portfolio.\n",
    "\n",
    "Instead of directly modeling $\\mathbb E(C / w \\mid \\boldsymbol x)$, pricing actuaries often decompose the pure premium into a product of frequency and severity $$\n",
    "  C / w = (C / w) \\cdot (N / N) = (N / w) \\cdot (C / N) = Y  Z$$ and derive two models: One for the claims frequency $$\n",
    "  \\mathbb E(Y \\mid \\boldsymbol x) \\approx m_Y(\\boldsymbol x),$$ classically a Poisson GLM with log link and case weights $w$; and another one for the severity $$\n",
    "  \\mathbb E(Z \\mid \\boldsymbol x) \\approx m_Z(\\boldsymbol x),$$ classically a Gamma GLM with log link and case weights $N$, using only rows with $N>0$.\n",
    "\n",
    "Assuming conditional independence of $Y$ and $Z$, the combined model for the pure premium is then given by the product of the two models, i.e., by $$\n",
    "  \\mathbb E(C / w \\mid \\boldsymbol x) \\approx m_Y(\\boldsymbol x)  m_Z(\\boldsymbol x).$$ The GLMs can be replaced by corresponding GAMs or modern ML techniques like deep learning and boosted trees. These alternatives are ideally fitted using the same loss functions (the corresponding deviance), the same case weights, and also using log links.\n",
    "\n",
    "For more information on non-life insurance pricing, we recommend the excellent references @wuethrich2021 and @ohlsson2015.\n",
    "\n",
    "## Further remarks\n",
    "\n",
    "-   The two models can use different feature sets.\n",
    "-   The severity model uses a log link to ensure a multiplicative model structure on the response scale. Since the logarithm is not the natural link function of the Gamma GLM, this leads to a slight bias on the scale of the response, which can be eliminated, e.g., by applying an empirical multiplicative correction factor $$\n",
    "    c = \\sum_{i = 1}^n y_i / \\sum_{i = 1}^n \\hat y_i$$ calculated on the training data.\n",
    "-   In R, when modeling claim frequencies with GLMs or GAMs, we will use the distribution family `quasipoisson`. This (a) suppresses a warning on non-integer responses and (b) allows for overdispersion. It provides the same parameter estimates as the `poisson` family.\n",
    "-   Instead of fitting a Poisson model for claims frequency using case weights $w$, one can alternatively model the claim counts $N$ by a Poisson model without case weights but using an offset of $\\log(w)$. Such a model will predict expected claim counts rather than expected claim frequencies. Nevertheless, the estimated effects will be identical between the two variants.\n",
    "\n",
    "# Main Example\n",
    "\n",
    "We will work with the French motor third-part liability (MTPL) dataset available on [openML (ID 41214)](https://www.openml.org/d/41214) to model claims frequency.\n",
    "\n",
    "## The data\n",
    "\n",
    "The first R script of this notebook performs the following steps:\n",
    "\n",
    "-   The dataset is downloaded from OpenML and saved.\n",
    "-   Rows with identical risk factors (\"Area\", \"VehPower\", \"VehAge\", \"DrivAge\", \"BonusMalus\", \"VehBrand\", \"VehGas\", \"Density\", \"Region\") are assigned to a \"group_id\". We assume that rows in such a group represent the same policy, even if their \"IDpol\" varies. This seems to be a curious property of this dataset. As we will see later, working with grouped data is a tricky business, and ignoring grouping can lead to biased results.\n",
    "-   Certain transformations are applied. For instance, very large feature values are truncated and small categories are collapsed to one category.\n",
    "-   The column names of the features, of the response and of the exposure are stored in variables.\n",
    "-   All relevant columns are described univariately.\n",
    "\n",
    "Note: We will often use the forward pipe operator `|>`. It moves the left-hand side as first argument in the function on the right-hand side. An example: `iris |> head()` is the same as `head(iris)`. If you need a quick refresher on R, go through [the first chapter](https://mayer79.github.io/statistical_computing_material/1_R_in_Action.html) of my statistical computing lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05676cf7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(patchwork)\n",
    "library(OpenML)\n",
    "library(farff)\n",
    "\n",
    "main <- \"french_motor\"\n",
    "# main <- \"r/french_motor\"\n",
    "fillc <- \"#E69F00\"\n",
    "refit <- TRUE # Set to FALSE after first run\n",
    "\n",
    "if (refit) {\n",
    "  # Download and save dataset\n",
    "  raw_file <- file.path(main, \"raw.rds\")\n",
    "  if (!file.exists(raw_file)) {\n",
    "    raw <- tibble(getOMLDataSet(data.id = 41214)$data)\n",
    "    saveRDS(raw, file = raw_file)\n",
    "  } else {\n",
    "    raw <- readRDS(raw_file)\n",
    "  }\n",
    "\n",
    "  # Preprocessing\n",
    "  prep <- raw |>\n",
    "    # Identify rows that might belong to the same policy\n",
    "    group_by_at(\n",
    "      c(\n",
    "        \"Area\", \"VehPower\", \"VehAge\", \"DrivAge\", \"BonusMalus\", \"VehBrand\",\n",
    "        \"VehGas\", \"Density\", \"Region\"\n",
    "      )\n",
    "    ) |>\n",
    "    mutate(group_id = cur_group_id(), group_size = n()) |>\n",
    "    ungroup() |>\n",
    "    arrange(group_id) |>\n",
    "    # Usual preprocessing\n",
    "    mutate(\n",
    "      Exposure = pmin(1, Exposure),\n",
    "      Freq = pmin(15, ClaimNb / Exposure),\n",
    "      VehPower = pmin(12, VehPower),\n",
    "      VehAge = pmin(20, VehAge),\n",
    "      VehGas = factor(VehGas),\n",
    "      DrivAge = pmin(85, DrivAge),\n",
    "      logDensity = log(Density),\n",
    "      VehBrand = relevel(fct_lump_n(VehBrand, n = 3), \"B12\"),\n",
    "      PolicyRegion = relevel(fct_lump_n(Region, n = 5), \"R24\"),\n",
    "      AreaCode = Area\n",
    "    )\n",
    "\n",
    "  # Covariates, response, weight\n",
    "  x <- c(\n",
    "    \"VehPower\", \"VehAge\", \"VehBrand\", \"VehGas\", \"DrivAge\",\n",
    "    \"logDensity\", \"PolicyRegion\"\n",
    "  )\n",
    "  y <- \"Freq\"\n",
    "  w <- \"Exposure\"\n",
    "} else {\n",
    "  load(file.path(main, \"intro.RData\"))\n",
    "}\n",
    "\n",
    "# Description\n",
    "nrow(prep)\n",
    "head(prep[, c(\"IDpol\", \"group_id\", \"ClaimNb\", x, w, y)], 8)\n",
    "summary(prep[, c(x, w, y)])\n",
    "\n",
    "# Histograms\n",
    "prep |>\n",
    "  select(Freq, Exposure, DrivAge, VehAge, VehPower, logDensity) |>\n",
    "  pivot_longer(everything()) |>\n",
    "  ggplot(aes(x = value)) +\n",
    "  geom_histogram(bins = 19, fill = fillc) +\n",
    "  facet_wrap(~name, scales = \"free\") +\n",
    "  ggtitle(\"Histograms of numeric columns\")\n",
    "\n",
    "# Barplots\n",
    "x_discrete <- c(\"ClaimNb\", \"VehBrand\", \"VehGas\", \"PolicyRegion\")\n",
    "one_barplot <- function(v) {\n",
    "  ggplot(prep, aes_string(v)) +\n",
    "    geom_bar(fill = fillc)\n",
    "}\n",
    "wrap_plots(lapply(x_discrete, one_barplot), top = \"Barplots of discrete columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdba399",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "-   The dataset has about 680k observations.\n",
    "-   The univariate distributions of the variables in the prepared dataset look plausible.\n",
    "-   Rows 3 and 4 share the same \"group_id\", as do rows 6, 7, and 8. Not only are all of their risk factors identical, but their \"IDpol\" are very similar as well. This increases confidence in our proxy \"group_id\".\n",
    "\n",
    "## The models\n",
    "\n",
    "We will now calculate three models with R:\n",
    "\n",
    "1.  An additive Poisson GLM fitted with the function `glm()` from the {stats} package, a\n",
    "2.  Poisson GAM with several penalized regression smooths with the `gam()` function of the {mgcv} package, and a\n",
    "3.  Poisson boosted trees model with implicit log link fitted by LightGBM [@ke2017] and tuned with grouped five-fold cross-validation.\n",
    "\n",
    "The models are calculated on about 80% of the observations, while the rest is kept for model validation. Since we suspect that some policies are represented by multiple rows, the data split is done in a grouped way, assigning all rows with the same \"group_id\" to either the training or the test dataset. That logic was also applied to create the cross-validation folds for selecting or *tuning* the hyperparameters of the LightGBM model. More about grouped splits will follow in Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26335596",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(splitTools)\n",
    "library(mgcv)\n",
    "library(lightgbm)\n",
    "\n",
    "set.seed(22) # Seeds the data split as well as the boosted trees model\n",
    "\n",
    "ind <- partition(prep$group_id, p = c(train = 0.8, test = 0.2), type = \"grouped\")\n",
    "train <- prep[ind$train, ]\n",
    "test <- prep[ind$test, ]\n",
    "\n",
    "# 1) GLM\n",
    "if (refit) {\n",
    "  fit_glm <- glm(\n",
    "    reformulate(x, y),\n",
    "    data = train, family = quasipoisson(), weights = train[[w]]\n",
    "  )\n",
    "}\n",
    "summary(fit_glm)\n",
    "\n",
    "# 2) GAM with penalized regression smooths and maximum k-1 df. Takes long to fit\n",
    "if (refit) {\n",
    "  fit_gam <- gam(\n",
    "    Freq ~ s(VehAge, k = 7) + s(DrivAge, k = 7) + s(logDensity, k = 3) +\n",
    "      s(VehPower, k = 3) + PolicyRegion + VehBrand + VehGas,\n",
    "    data = train,\n",
    "    family = quasipoisson(),\n",
    "    weights = train[[w]]\n",
    "  )\n",
    "}\n",
    "summary(fit_gam)\n",
    "\n",
    "# Visualizing the effect of an additive component on the scale of the linear predictor\n",
    "plot(fit_gam, select = 2, main = \"Additive effect of 'DrivAge' on log scale\")\n",
    "\n",
    "# 3) Boosted trees\n",
    "if (refit) {\n",
    "  # Data interface of LightGBM (LGM)\n",
    "  dtrain <- lgb.Dataset(\n",
    "    data.matrix(train[x]),\n",
    "    label = train[[y]],\n",
    "    weight = train[[w]],\n",
    "    params = list(feature_pre_filter = FALSE)\n",
    "  )\n",
    "\n",
    "  # Parameters found by CV randomized search, see later\n",
    "  params <- list(\n",
    "    learning_rate = 0.05,\n",
    "    objective = \"poisson\",\n",
    "    metric = \"poisson\",\n",
    "    num_leaves = 63,\n",
    "    min_data_in_leaf = 100,\n",
    "    min_sum_hessian_in_leaf = 0,\n",
    "    colsample_bynode = 0.8,\n",
    "    bagging_fraction = 0.8,\n",
    "    lambda_l1 = 4,\n",
    "    lambda_l2 = 0,\n",
    "    num_threads = 7 # adapt\n",
    "  )\n",
    "\n",
    "  # Fit model (\"nrounds\" is an optimized parameter as well, see later)\n",
    "  fit_lgb <- lgb.train(params = params, data = dtrain, nrounds = 174)\n",
    "  lgb.save(fit_lgb, file.path(main, \"fit_lgb.txt\"))\n",
    "} else {\n",
    "  fit_lgb <- lgb.load(file.path(main, \"fit_lgb.txt\"))\n",
    "}\n",
    "\n",
    "# Save everything important\n",
    "if (refit) {\n",
    "  save(x, y, w, fit_glm, fit_gam, prep, train, test, fillc,\n",
    "    file = file.path(main, \"intro.RData\")\n",
    "  )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a76c76",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "-   Fitting a GAM on 540k rows of data takes comparably long.\n",
    "-   The code required to fit the boosted trees model is much longer than for the GLM and the GAM. However, the code is very general, so that only minor adaptions need to be made for other modeling situations.\n",
    "-   Interpreting the GLM and the GAM can be easily made from the model output. For example, for the GLM, a one-point increase in \"DrivAge\" is associated with a change in log expected claims frequency of $-0.0051$. On the frequency scale, this corresponds to a change of $100\\% \\cdot (e^{-0.0051} - 1) \\approx -0.5\\%$. The GAM shows a more complex (and realistic) age effect. For the GLM and the GAM, the test statistics give us an indication of the variable importance: Vehicle age and driver age appear to be among the strongest predictors. What about the boosted trees model? It will be one of the goals of the next chapter to fill this gap using the tools of XAI.\n",
    "\n",
    "# Exercises\n",
    "\n",
    "Here, we will work with an Australian car claims dataset \"dataCar\" available in the R package {insuranceData}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1657d1",
   "metadata": {
    "eval": false,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(insuranceData)\n",
    "\n",
    "data(dataCar)\n",
    "head(dataCar)\n",
    "\n",
    "# Data preparation\n",
    "prep <- dataCar |>\n",
    "  mutate(\n",
    "    Freq = numclaims / exposure,\n",
    "    Sev = claimcst0 / numclaims,\n",
    "    veh_age = factor(veh_age),\n",
    "    agecat = factor(agecat),\n",
    "    veh_body = fct_lump_prop(veh_body, 0.1),\n",
    "    log_value = log(pmax(0.3, pmin(15, veh_value)))\n",
    "  )\n",
    "\n",
    "# Variable groups\n",
    "y_var <- \"Freq\"\n",
    "w_var <- \"exposure\"\n",
    "x_vars <- c(\"log_value\", \"agecat\", \"veh_age\", \"area\", \"veh_body\", \"gender\")\n",
    "\n",
    "# XGBoost parameters for the last exercise\n",
    "params <- list(\n",
    "  learning_rate = 0.05,\n",
    "  objective = \"count:poisson\",\n",
    "  max_depth = 2,\n",
    "  colsample_bynode = 0.8,\n",
    "  subsample = 0.8,\n",
    "  reg_alpha = 1,\n",
    "  reg_lambda = 0,\n",
    "  min_split_loss = 0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46d912",
   "metadata": {},
   "source": [
    "1.  Descriptive Analysis\n",
    "    i.  Study the data description via `?dataCar`.\n",
    "    ii. Calculate relevant statistics at portfolio level, i.e., on the full dataset: Total and average exposure, total claim amount, number of claims, pure premium, frequency, and severity.\n",
    "    iii. Describe the distribution of each model feature in `x_var` in the prepared dataset. Use tables and/or plots and/or text.\n",
    "2.  Modeling: Next, we develop different claims frequency models using the same set of features (`x_vars`).\n",
    "    i.  Split the dataset into 80% for training and 20% for testing. Don't touch the test data during modeling.\n",
    "    ii. GLM: Fit an exposure-weighted Poisson GLM with log link to the claim frequencies. Interpret the estimated coefficients of the features \"log_value\" and \"gender\".\n",
    "    iii. Random forest: Model \"Freq\" by an exposure-weighted random forest with the R package {ranger} using the quadratic loss. Select the optimal tree depth by minimizing exposure-weighted Poisson deviance calculated from the out-of-bag (OOB) predictions on the training data. How are OOB predictions being calculated? You can use the function `deviance_poisson()` in the {MetricsWeighted} package to calculate deviance.\n",
    "    iv. XGBoost: Study the documentation of [XGBoost](https://xgboost.readthedocs.io/en/stable/). The original publication is @chen2016, and a fantastic slide deck (pdf) of the author can be found on this [Link](https://web.njit.edu/~usman/courses/cs675_fall16/BoostedTree.pdf). Use the {xgboost} package to fit an exposure-weighted Poisson XGBoost model with 196 boosting rounds and the hyperparameters stated in the code above. We will later see *how* to find such parameters. Note that XGBoost automatically uses a log link when fitting a Poisson model.\n",
    "\n",
    "# References"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "name,eval,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
